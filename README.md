# KD
## Area for research: 
1. Data-free KD 
2. Multi-modality KD 
3. KD for generative models
4. KD for segmentation (tasks other than classification are tackled less)
## Papers from [CVPR2023](https://cvpr2023.thecvf.com/Conferences/2023/AcceptedPapers): 
20 papers (might missed some papers)
| Title      | 
| ----------- |
|[Constructing Deep Spiking Neural Networks from Artificial Neural Networks with Knowledge Distillation](https://arxiv.org/abs/2304.05627)|
|[Masked Autoencoders Enable Efficient Knowledge Distillers](https://arxiv.org/abs/2208.12256)|
|[Multi-Mode Online Knowledge Distillation for Self-Supervised Visual Representation Learning](https://arxiv.org/abs/2304.06461)  |
|[StructVPR: Distill Structural Knowledge with Weighting Samples for Visual Place Recognition](https://arxiv.org/abs/2212.00937)  |
|[PointDistiller: Structured Knowledge Distillation Towards Efficient and Compact 3D Detection](https://arxiv.org/abs/2205.11098)  |
|[KD-GAN: Data Limited Image Generation via Knowledge Distillation](https://arxiv.org/abs/2303.17158)  |
|[Generalization Matters: Loss Minima Flattening via Parameter Hybridization for Efficient Online Knowledge Distillation](https://arxiv.org/abs/2303.14666)  |
|[Knowledge Distillation for 6D Pose Estimation by Aligning Distributions of Local Predictions](https://arxiv.org/abs/2205.14971)  |
|[Rethinking Feature-based Knowledge Distillation for Face Recognition]()  - unavilabel  |
|[Supervised Masked Knowledge Distillation for Few-Shot Transformers ](https://arxiv.org/abs/2303.15466)  |
|[Data-Free Knowledge Distillation via Feature Exchange and Activation Region Constraint]()  - unavilabel  |
|[Class Attention Transfer Based Knowledge Distillation](https://arxiv.org/pdf/2304.12777.pdf)  |
|[A Unified Knowledge Distillation Framework for Deep Directed Graphical Models](https://openreview.net/pdf?id=IxCAF8IMatf)  |
|[X^3KD: Knowledge Distillation Across Modalities, Tasks and Stages for Multi-Camera 3D Object Detection](https://arxiv.org/abs/2303.02203)  |
|[UniDistill: A Universal Cross-Modality Knowledge Distillation Framework for 3D Object Detection in Birdâ€™s-Eye View](https://arxiv.org/pdf/2303.15083.pdf)  |
|[Learning to Retain while Acquiring: Combating Distribution-Shift in Adversarial Data-Free Knowledge Distillation](https://arxiv.org/pdf/2302.14290)  |
|[Incrementer: Transformer for Class-Incremental Semantic Segmentation with Knowledge Distillation Focusing on Old Class]()  - unavilabel  |
|[MobileVOS: Real-Time Video Object Segmentation Contrastive Learning meets Knowledge Distillation](https://arxiv.org/pdf/2303.07815)  |
|[itKD: Interchange Transfer-based Knowledge Distillation for 3D Object Detection](https://arxiv.org/pdf/2205.15531)  |
|[Probabilistic Knowledge Distillation for Face Ensembles]()  - unavilabel  |
|[DaFKD: Domain-aware Federated Knowledge Distillation](https://www.preprints.org/manuscript/202303.0432/download/final_file)  |
