Data loaded: there are 2555428 images.
model and optimizer are ready.
Starting clippo training !
Traceback (most recent call last):
  File "/home/temp/Desktop/KD/CLIPPO/train.py", line 152, in <module>
    main(args)
  File "/home/temp/Desktop/KD/CLIPPO/train.py", line 112, in main
    loss = train_one_epoch(clippo, data_loader, optimizer,
  File "/home/temp/Desktop/KD/CLIPPO/train.py", line 21, in train_one_epoch
    loss = clippo(image=images, text=text)
  File "/home/temp/miniconda3/envs/stanford_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/temp/Desktop/KD/CLIPPO/network.py", line 13, in forward
    image_features = self.encoder(image)
  File "/home/temp/miniconda3/envs/stanford_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/temp/miniconda3/envs/stanford_env/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 549, in forward
    x = self.forward_features(x)
  File "/home/temp/miniconda3/envs/stanford_env/lib/python3.9/site-packages/timm/models/vision_transformer.py", line 532, in forward_features
    x = self.patch_embed(x)
  File "/home/temp/miniconda3/envs/stanford_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/temp/miniconda3/envs/stanford_env/lib/python3.9/site-packages/timm/models/layers/patch_embed.py", line 44, in forward
    x = self.proj(x)
  File "/home/temp/miniconda3/envs/stanford_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/temp/miniconda3/envs/stanford_env/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/temp/miniconda3/envs/stanford_env/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same