Data loaded: there are 60000 images.
model and optimizer are ready.
Starting clippo training !
torch.Size([1100, 3, 32, 32])
tensor([9, 7, 8,  ..., 0, 8, 2], device='cuda:0')
torch.Size([1100])
/home/temp/Desktop/KD/CLIPPO/train.py:28: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/TensorShape.cpp:3277.)
  logits_per_image, logits_per_text = clippo(image=images, text=text.T)
/home/temp/miniconda3/envs/stanford_env/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
Traceback (most recent call last):
  File "/home/temp/Desktop/KD/CLIPPO/train.py", line 168, in <module>
    main(args)
  File "/home/temp/Desktop/KD/CLIPPO/train.py", line 126, in main
    loss = train_one_epoch(clippo, data_loader, optimizer,
  File "/home/temp/Desktop/KD/CLIPPO/train.py", line 28, in train_one_epoch
    logits_per_image, logits_per_text = clippo(image=images, text=text.T)
  File "/home/temp/miniconda3/envs/stanford_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/temp/Desktop/KD/CLIPPO/network.py", line 22, in forward
    text_features = self.text_proj (self.encoder(text))
  File "/home/temp/miniconda3/envs/stanford_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/temp/miniconda3/envs/stanford_env/lib/python3.9/site-packages/timm/models/resnet.py", line 730, in forward
    x = self.forward_features(x)
  File "/home/temp/miniconda3/envs/stanford_env/lib/python3.9/site-packages/timm/models/resnet.py", line 709, in forward_features
    x = self.conv1(x)
  File "/home/temp/miniconda3/envs/stanford_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/temp/miniconda3/envs/stanford_env/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/temp/miniconda3/envs/stanford_env/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1100]